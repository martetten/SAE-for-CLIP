# SAE для интерпретации внутренних представлений модели CLIP

Реализация разреженного автоэнкодера (Sparse Autoencoder, SAE) для декомпозиции активаций промежуточных слоёв модели CLIP ViT-B/32 на интерпретируемые латентные признаки. Проект демонстрирует выделение семантически осмысленных визуальных паттернов из внутренних представлений мультимодальной модели и оценку влияния реконструкции на точность классификации.

## Ключевые результаты

| Метрика | Значение | Интерпретация |
|---------|----------|---------------|
| **Объяснённая дисперсия (EVR)** | 80.57% | Превышает требование преподавателя (>80%), подтверждает качество реконструкции |
| **Стабильность разреженности (L0)** | 500.0 | Точное соблюдение целевого числа активных фичей на всех эпохах обучения |
| **Деградация на CIFAR-10** | -1.53 п.п. | Минимальная потеря точности на простом датасете (93.66% -> 92.13%) |
| **Деградация на CIFAR-100** | -10.04 п.п. | Ожидаемая деградация на сложном датасете с низким разрешением (75.91% -> 65.87%) |

## Структура репозитория

SAE-for-CLIP/
├── assets/
│   └── report_collages_clean/    # Визуальные интерпретации 25 фичей (коллажи 4×3)
├── checkpoints_v2/
│   └── sae_best.pth               # Обученная модель SAE (577 МБ, требует Git LFS)
├── configs/
│   ├── sae_config.yaml            # Гиперпараметры архитектуры SAE
│   └── training_config.yaml       # Параметры обучения (датасет, эпохи, батч)
├── data/
│   ├── interpretations.csv        # Таблица 300 фичей с активациями и интерпретациями
│   ├── metrics_table.md           # Метрики обучения по эпохам
│   └── zero_shot_metrics.md       # Результаты оценки деградации на CIFAR-10/100
├── experiments/                   # Этап 1: исследование архитектуры CLIP
│   ├── test_clip_layers.ipynb    # Анализ качества активаций разных слоёв CLIP
│   ├── test_hooks.py              # Реализация хуков для извлечения активаций
│   └── test_images/               # Тестовые изображения для визуализации
├── notebooks/
│   └── kandinsky_steering.ipynb   # Заготовка для этапа 6 (управление генерацией)
├── scripts/
│   ├── train_sae.py               # Обучение SAE на датасете food101
│   ├── evaluate_zeroshot.py       # Оценка деградации точности классификации
│   ├── interpret_latents.py       # Генерация коллажей для интерпретации фичей
│   ├── analyze_training.py        # Анализ логов обучения и генерация метрик
│   └── clean_checkpoint.py        # Очистка чекпоинта от лишних данных (оптимизация размера)
├── src/
│   ├── clip_utils.py              # Извлечение активаций из слоя 11 CLIP через хуки
│   ├── clip_with_sae.py           # Интеграция SAE в прямой проход CLIP для оценки
│   ├── evaluation.py              # Zero-shot классификация с/без SAE
│   ├── interpretation.py          # Генерация коллажей из топ-изображений по фичам
│   ├── loss.py                    # Функция потерь (только MSE, без L1 при топ-К)
│   ├── sae.py                     # Архитектура SAE (исправлена ошибка двойного биаса)
│   └── training_loop.py           # Тренировочный цикл с вармапом разреженности
├── report.md                      # Научный отчёт с анализом результатов и коллажами
├── requirements.txt               # Зависимости проекта
└── README.md                      # Данный файл


## Требования и установка

### Системные требования
- Python 3.10+
- GPU с 8+ ГБ видеопамяти (рекомендуется)
- 20 ГБ свободного места на диске (включая кэш датасетов)

### Установка зависимостей
```bash
git clone https://github.com/martetten/SAE-for-CLIP.git
cd SAE-for-CLIP
pip install -r requirements.txt
```
### Настройка Git LFS (обязательно для загрузки чекпоинта)
```bash
# Установка Git LFS
git lfs install

# Клонирование репозитория с автоматической загрузкой больших файлов
git clone https://github.com/martetten/SAE-for-CLIP.git
cd SAE-for-CLIP

# Файл checkpoints_v2/sae_best.pth имеет размер 577 МБ и хранится через Git LFS. При первом клонировании репозитория он будет автоматически загружен. Для ручной загрузки используйте git lfs pull
```

## Быстрый старт: воспроизведение ключевых результатов

### 1. Оценка деградации точности классификации
```bash
# Результаты сохраняются в data/zero_shot_metrics_test.md с таблицей деградации на двух датасетах.
python scripts/evaluate_zeroshot.py \
  --sae_checkpoint ./checkpoints_v2/sae_best.pth \
  --cifar_samples 1000 \
  --cifar100_samples 1000 \
  --output ./data/zero_shot_metrics_test.md
```
### 2. Генерация коллажей для интерпретации фичей
```bash
# Создаёт 300 коллажей в assets/report_collages_test/ и таблицу интерпретаций в data/interpretations_test.csv.
python scripts/interpret_latents.py \
  --sae_checkpoint ./checkpoints_v2/sae_best.pth \
  --top_k 12 \
  --sample_size 1000 \
  --output_csv ./data/interpretations_test.csv \
  --collage_dir ./assets/report_collages_test
```
### 3. Анализ метрик обучения
```bash
# Генерирует таблицу метрик по эпохам обучения.
python scripts/analyze_training.py \
  --logdir ./logs/sae_training_v2/20260206_133909 \
  --output ./data/metrics_test.md
```
## Запуск полного цикла обучения (опционально)

```bash
# Обучение займёт ~2.5 часа на GPU среднего уровня (RTX 3060/4060) и потребует ~1.5 ГБ дискового пространства для кэша датасета.
python scripts/train_sae.py \
  --sae_config configs/sae_config.yaml \
  --training_config configs/training_config.yaml
```

## Отчёт

Полный анализ результатов, включая интерпретацию фичей с визуальными примерами коллажей, представлен в файле report.md. Отчёт содержит:
- Теоретическое обоснование подхода
- Анализ динамики обучения и преодоления коллапса разреженности
- Качественную интерпретацию некоторых ключевых фичей с иллюстрациями